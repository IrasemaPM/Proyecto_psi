[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "El valor optimo del indice de comportamiento",
    "section": "",
    "text": "Prefacio\nEn este libro se desarrollara el proyecto de la clase de Aprendizaje Reforzado.",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introdución",
    "section": "",
    "text": "En la literatura existen una variedad de modelos tipo SIRS en los cuales consideran la vacunación como medida de prevención para el control de enfermedades (Kribs-Zaleta y Velasco-Hernández 2000) . Sin embargo, este medio de control llega a presentar algunas fallas como son: la falla de grado, falla en la toma y falla en la duración (McLean y Blower 1993). En la modelación de enfermedades respiratorias con ecuaciones diferenciales, se suele considerar que las vacunas tienen una falla tipo de grado (Arino, McCluskey, y Driessche 2003). Por otro lado, (Pedroza-Meza, Velasco-Hernández, y Acuña-Zegarra 2025), proponen un modelo de ecuaciones diferenciales donde consideran que la vacuna tiene los tre tipos de fallas.\nEn este ultimo trabajo, proponen un índice de comportamiento (\\(\\psi\\)) el cual permite medir que tan riguroso pueden seguir las medidas de prevención una vez que son vacunados. En el modelo que proponen este índice solo afecta a los vacunados no inmunes ya que supone que los vacunados que generan inmunidad no se pueden infectar. En este trabajo se desarrollará un proceso de decisión de Markov diseñado para identificar el conjunto de valores de \\(\\psi\\) que minimice la cantidad de infectados al finalizar un año (Sutton 2018).\n\n\n\n\nArino, Julien, C Connell McCluskey, y Pauline van den Driessche. 2003. «Global results for an epidemic model with vaccination that exhibits backward bifurcation». SIAM Journal on Applied Mathematics 64 (1): 260-76.\n\n\nKribs-Zaleta, Christopher M, y Jorge X Velasco-Hernández. 2000. «A simple vaccination model with multiple endemic states». Mathematical biosciences 164 (2): 183-201.\n\n\nMcLean, Angela Ruth, y Sally M Blower. 1993. «Imperfect vaccines and herd immunity to HIV». Proceedings of the Royal Society of London. Series B: Biological Sciences 253 (1336): 9-13.\n\n\nPedroza-Meza, Irasema, Jorge X Velasco-Hernández, y M Adrian Acuña-Zegarra. 2025. «Multiple endemic equilibria in the presence of imperfect vaccine coverage». Mathematical biosciences 164 (2).\n\n\nSutton, Richard S. 2018. «Reinforcement learning: An introduction». A Bradford Book.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introdución</span>"
    ]
  },
  {
    "objectID": "Formulacion.html",
    "href": "Formulacion.html",
    "title": "2  Formulación del Proceso de Decisión de Markov",
    "section": "",
    "text": "Los estados de nuestro proceso de decisión de Markov representarán la proporción de la población en cada categoría del modelo propuesto por (Pedroza-Meza, Velasco-Hernández, y Acuña-Zegarra 2025):\n\n\\(S_{t}\\): Fracción de susceptibles en el tiempo \\(t\\).\n\\(V_{+t}\\): Fracción de vacunados inmunes en el tiempo \\(t\\).\n\\(V_{-t}\\): Fracción de vacunados no inmunes en el tiempo \\(t\\).\n\\(I_{t}\\): Fracción de infectados en el tiempo \\(t\\).\n\\(R_{t}\\): Fracción de recuperados en el tiempo \\(t\\).\n\nEl estado global del sistema en el tiempo \\(t\\) se presenta como\n\\[\nx_{t} = (S_{t}, V_{+t}, V_{-t}, I_{t}, R_{t})\n\\]\nEl escenario que consideraremos para cada \\(t \\in {0,1,\\dots, N}\\)para el proceso de Markov:\n\n\\(x_{t}\\): representa la dinámica de la enfermedad en el tiempo \\(t\\).\n\\(a_{t}\\): representa en qué escenario del índice de comportamiento se encuentra la población en el tiempo\\(t\\).\n\nAlgunos supuestos que estaremos considerando para nuestro proceso son:\n\nLas personas cambian su comportamiento en el tiempo \\(t\\) de forma instantánea.\nLas únicas personas que pueden cambiar su comportamiento son los vacunados no inmunes.\nSupondremos que las personas cambian su comportamiento bajo una distribución uniforme \\([0.5, 2]\\).\n\nBajo los supuestos anteriormente mencionados, consideramos el siguiente Modelo de Control de Markov.\n\\[\n(\\mathbf{X},  \\{A(x): x \\in X\\}, \\mathbf{P}, \\mathbf{C})\n\\]\ndonde \\(\\mathbf{X}\\) es el espacio de los estados, \\(\\{A(x): x \\in X\\}\\) es el espacio de las acciones admisibles, \\(\\mathbf{P}\\) es la ley de transicion de modelo y \\(\\mathbf{C}\\) es la funcion de costo.\nEn la siguiente figura se ilustra el cambio de un estado a otro por las acciones admisibles. Desarrollaremos la dinámica de la cadena de Markov en las siguientes secciones.\n\n\n\nDiagrama del cambio de un estado a otro.\n\n\n\n\n\n\nPedroza-Meza, Irasema, Jorge X Velasco-Hernández, y M Adrian Acuña-Zegarra. 2025. «Multiple endemic equilibria in the presence of imperfect vaccine coverage». Mathematical biosciences 164 (2).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Formulación del Proceso de Decisión de Markov</span>"
    ]
  },
  {
    "objectID": "DinamicaModel.html",
    "href": "DinamicaModel.html",
    "title": "3  Dinámica del Modelo",
    "section": "",
    "text": "En esta sección veremos cómo evoluciona el sistema atreves del tiempo \\(t\\). La dinámica de la enfermedad esta representando por la siguiente cadena de Markov\n\n\\(S_{t+1}= \\mu -\\beta S_{t}I_{t}- (\\mu + \\phi)S_{t}+\\omega V_{+t}+\\xi R_{t}\\)\n\\(V_{+t+1}= \\phi\\_{+}(\\sigma)S_{t}-(\\mu+\\omega) V_{+t}\\)\n\\(V_{-t+1}=\\phi_{-}(\\sigma)S_{t}- \\psi(1-\\sigma) \\beta V_{-t} I_{t}-\\mu V_{-t}\\)\n\\(I_{t+1}= \\beta S_{t}I_{t}+\\psi(1-\\sigma) \\beta V_{-t} I_{t} -(\\mu+ \\gamma)I_{t}\\)\n\\(R_{t+1}=\\gamma I_{t}-(\\mu +\\xi) R_{t}\\)\n\nEn la siguiente figura es una representacion de la cadena de Markov descrita anteriormente.\n\n\n\nDiagrama de la cadena de Markov en el tiempo t\n\n\nLas acciones representan los valores del índice de comportamiento \\((\\psi)\\). Supondremos que los valores del índice de comportamiento están restringidos al intervalo \\([0.5,2]\\). Las acciones consideradas en nuestro modelo están dividas en tres principales acciones:\n\nSi \\(\\psi\\in [0.5,0.9)\\) entonces diremos que las personas se portan bien, es decir, que las personas siguen las medidas de prevención de forma estricta.\nSi \\(\\psi\\in [0.9,1.2]\\) entonces diremos que las personas se portan normal, es decir, que las personas siguen las medidas de prevención.\nSi \\(\\psi\\in (1.2,2]\\) entonces diremos que las personas se portan mal, es decir, que las personas no siguen las medidas de prevención.\n\nSupondremos que las personas toman decisiones diarias mediante unas distribución \\(x_{t} \\backsim Uni[0.5,2]\\) el periodo de observación será de un año. Como se ha mencionado con anterioridad el objetivo de este trabajo es encontrar el valor de \\(\\psi\\) que permita tener la incidencia acumulada más pequeña al final del año. Entonces la probabilidad de tener \\(i\\) de incidencia acumulada y cambiar el comportamiento \\(\\alpha\\) en el tiempo \\(t\\) y pasar al estado \\(j\\) está dada por: \\[P_{ij}(\\alpha)=P[x_{t+1}=j|x_{t}=i,a_{t}=\\alpha]\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dinámica del Modelo</span>"
    ]
  },
  {
    "objectID": "costo.html",
    "href": "costo.html",
    "title": "4  Descripción y justificación del costo",
    "section": "",
    "text": "En las secciones anteriores hemos mencionado que el objetivo es disminuir la incidencia acumulada en el tiempo \\(t=365\\). Por lo tanto, proponemos que la función de costo este dado por el costo de la enfermedad \\((C_{I})\\) y costo de vacunas con falla en la toma \\((C_{V})\\) sea\n\\[C_{I}(t)=c_{i}I_{t}\\] donde \\(c_{i}\\) es el costo unitario por persona infectada y \\(I_{t}\\)\n\\[C_{V-}(t)= c_{v}\\phi_{-} V_{-}\\] \\(c_{v}\\) es el costo de generar vacunas con falla en la toma y \\(\\phi_{-} S\\) es el número de susceptibles vacunados que no generan inmunidad. Ahora podemos calcular el impacto global que es: \\[C(t)=C_{I}(t)+C_{V-}(t)\\]\nEntonces el costo total esta dado por \\(C_{total}=\\sum_{t=0}^{365} (0.5)^{2}C(t)\\)\nLas recompensas estarán determinadas mediante las acciones tomadas en el estado anterior. En la siguiente sección profundizaremos sobre las acciones y las recompensas que proponemos que se consideren esta clase de modelos.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Descripción y justificación del costo</span>"
    ]
  },
  {
    "objectID": "acciones.html",
    "href": "acciones.html",
    "title": "5  Justificación de las acciones",
    "section": "",
    "text": "En la sección 3 se observo como el conjunto principal de acciones que se consideran esta dada por intervalos. Consideramos que los valores de \\(\\psi\\) va aumentando o disminuyendo en \\(0.1\\). Lo que nos lleva\n\nPortarse bien: serán los valores cuando \\(\\psi=0.5,0.6,0.7\\) o \\(\\psi=0.8\\)\nPortarse normal: serán los valores cuando \\(\\psi=0.9,1,1.2\\)\nPortarse mal: serán los valores cuando \\(\\psi=1.3,1.4,\\dots\\) o \\(\\psi=2\\)\n\nCada una de estas acciones deben tener una recompensa y estara dada por la siguiente tabla\n\n\n\n\n\n\n\n\n\\[                 \n  X_{t}              \n  \\]\n\\[                                   \n                                         X_{t+1}            \n                                         \\]\nRecompesa\n\n\n\n\nPortarse bien\nPortarse bien\n-0.5\n\n\nPortarse bien\nPortarse normal\n1\n\n\nPortarse bien\nPortarse mal\n2\n\n\nPortarse normal\nPortarse bien\n0.5\n\n\nPortarse normal\nPortarse normal\n1\n\n\nPortarse normal\nPortarse mal\n-0.5\n\n\nPortarse mal\nPortarse bien\n1\n\n\nPortarse mal\nPortarse normal\n0.5\n\n\nPortarse mal\nPortarse mal\n-1",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Justificación de las acciones</span>"
    ]
  },
  {
    "objectID": "Simulacion.html",
    "href": "Simulacion.html",
    "title": "6  Simulaciones del proceso de Markov",
    "section": "",
    "text": "Hemos realizado simulaciones utilizando el programa R, el cual nos permitirá visualizar el proceso de toma de decisiones en cada caso, i.e., si las personas siguieron o abandonaron las medidas de prevención. Además, mostraremos cómo son las recompensas asociadas a estas acciones.\nPrimero, definiremos la función que almacenará nuestro sistema de ecuaciones diferenciales.\nEDO_SVVIR_Normalizado &lt;- function(Time, State, Pars) {\n  with(as.list(c(State, Pars)), {\n    dS &lt;- mu - (vbeta * S * I) - (mu + phi) * S + (omega * V_plus) + (xi * R)\n    dV_plus &lt;- phi_plus * S - (mu + omega) * V_plus\n    dV_minus &lt;- phi_minus * S - ((c * vbeta) * (V_minus * I)) - mu * V_minus\n    dI &lt;- (vbeta * S * I) + ((c * vbeta) * (V_minus * I)) - (mu + vgamma) * I\n    dR &lt;- (vgamma * I) - (mu + xi) * R\n\n    return(list(c(dS, dV_plus, dV_minus, dI, dR)))\n  })\n}\nAhora definiremos los valores de los parámetros necesarios para el sistema de ecuaciones diferenciales\n# Limpiar la memoria de las variables\nrm(list = ls())\n\n# Paquetes necesarios para simulación\nlibrary(deSolve)\nlibrary(plotly)\n\n# Sistema de ecuaciones\nsource(\"EDO_SVVIR_Normalizado.R\", local = FALSE)\n\n# Parámetros iniciales\nmu &lt;- 1 / (70 * 365)\nomega &lt;- 0.02\nvgamma &lt;- 1 / 14\np &lt;- 0.7\nvsigma &lt;- 0.75\np_e &lt;- p * vsigma\nvT &lt;- 35\nphi_plus &lt;- (-log(1 - p_e)) / vT\nphi_minus &lt;- (1 / vT) * log((1 - p_e) / (1 - p))\nphi &lt;- phi_plus + phi_minus\nxi &lt;- 1 / 30\nvbeta &lt;- 0.270\nEn el siguiente código, mostraremos los pasos a seguir para realizar las simulaciones del proceso de Markov.\n# Simulaciones del modelo\nfor (k in 1:n) {\n  t0 &lt;- 0\n  T0 &lt;- 65\n  t &lt;- 1\n  RejillaTiempo &lt;- seq(from = t0, to = T0, by = 1 / t)\n  \n  vecTime &lt;- numeric()\n  velec &lt;- character()\n  vpsi &lt;- numeric()\n  \n  psi0 &lt;- runif(1, 0.5, 2)\n  vpsi[1] &lt;- psi0\n  c &lt;- (1 - vsigma) * psi0\n  \n  vpar &lt;- c(\n    vbeta = vbeta, mu = mu, phi = phi, omega = omega,\n    xi = xi, phi_plus = phi_plus, phi_minus = phi_minus,\n    c = c, vgamma = vgamma\n  )\n  \n  outSVVIR &lt;- ode(yinicial, RejillaTiempo, EDO_SVVIR_Normalizado, vpar, method = \"lsoda\")\n  vPI &lt;- numeric()\n  vPI[1] &lt;- outSVVIR[T0, 5] * 10000\n  \n  velec[1] &lt;- if (0.5 &lt; psi0 && psi0 &lt; 0.9) {\n    \"bien\"\n  } else if (0.9 &lt;= psi0 && psi0 &lt;= 1.2) {\n    \"normal\"\n  } else {\n    \"mal\"\n  }\n  \n  vecTime[1] &lt;- T0\n  \n  for (i in 1:3) {\n    T0 &lt;- 65 + (100 * i)\n    RejillaTiempo &lt;- seq(from = t0, to = T0, by = 1 / t)\n    vecTime[i + 1] &lt;- T0\n    \n    psi0 &lt;- runif(1, 0.5, 2)\n    c &lt;- (1 - vsigma) * psi0\n    vpsi[i + 1] &lt;- psi0\n    \n    vpar &lt;- c(\n      vbeta = vbeta, mu = mu, phi = phi, omega = omega,\n      xi = xi, phi_plus = phi_plus, phi_minus = phi_minus,\n      c = c, vgamma = vgamma\n    )\n    \n    outSVVIR &lt;- ode(yinicial, RejillaTiempo, EDO_SVVIR_Normalizado, vpar, method = \"lsoda\")\n    vPI[i + 1] &lt;- outSVVIR[T0, 5] * 10000\n    \n    velec[i + 1] &lt;- if (0.5 &lt; psi0 && psi0 &lt; 0.9) {\n      \"bien\"\n    } else if (0.9 &lt;= psi0 && psi0 &lt;= 1.2) {\n      \"normal\"\n    } else {\n      \"mal\"\n    }\n  }\n  \n  MPI[, k] &lt;- vPI\n  Mpsi[, k] &lt;- vpsi\n  Mnames[, k] &lt;- velec\n}\nSiguiendo los códigos anteriores, podemos desarrollar las simulaciones necesarias para ilustrar la dinámica del proceso de Markov. En el siguiente escenario, se toma una decisión cada 100 días durante un año, y cada simulación representa un año diferente. En la siguiente gráfica, se muestran los diferentes valores de \\(\\psi\\) asociados a las decisiones tomadas por las personas, ya sea seguir o no las medidas de prevención\n\n\n\nGraficas de los valores de \\(\\psi\\).\n\n\nEn la siguiente gráfica, mostramos cómo serían las recompensas correspondientes a cada una de las decisiones tomadas. Estas recompensas están relacionadas con la tabla presentada en la Sección 5 de este documento.\n\n\n\nSe muetra las diferentes recompensas para cada uno de los valores de \\(\\psi\\).\n\n\nFinalmente, en esta última figura se muestran las consecuencias de tomar ese conjunto de decisiones en la prevalencia de la enfermedad.\n\n\n\nLa prevalencia de la enfermedad en cada año.\n\n\nCon las Figuras anteriores se puede observar como el año 2 y 3 pueden terne la misma recompensa pero en la selecion de los valores de \\(\\psi\\) son diferentes.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Simulaciones del proceso de Markov</span>"
    ]
  }
]